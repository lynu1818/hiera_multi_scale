[09/30 22:44:33] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 22:44:34] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 22:44:34] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 22:44:34] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe//logs/in1k_mae/mae_hiera_abs_win_tiny_512/epoch-epoch=99.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 22:44:34] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 22:44:34] d2.utils.env INFO: Using a generated random seed 38542550
[09/30 22:44:35] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 22:44:35] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 22:44:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 22:44:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 22:44:37] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 22:44:37] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 22:44:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 22:44:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 22:44:37] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 22:44:38] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 22:44:38] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 22:44:38] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 22:52:05] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 22:52:06] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 22:52:06] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 22:52:06] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 22:52:06] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 22:52:06] d2.utils.env INFO: Using a generated random seed 10175291
[09/30 22:52:07] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 22:52:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 22:52:07] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 22:52:07] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 22:52:08] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 22:52:08] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 22:52:08] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 22:52:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 22:52:08] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 22:52:08] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 22:52:08] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 22:52:09] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 23:01:34] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 23:01:35] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 23:01:35] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 23:01:35] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 23:01:35] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 23:01:35] d2.utils.env INFO: Using a generated random seed 39570163
[09/30 23:01:36] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:01:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:01:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 23:01:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 23:01:37] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 23:01:37] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 23:01:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 23:01:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 23:01:37] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 23:01:37] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 23:01:37] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 23:01:38] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 23:02:42] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 23:02:43] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 23:02:43] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 23:02:43] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 23:02:43] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 23:02:43] d2.utils.env INFO: Using a generated random seed 47399157
[09/30 23:02:44] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:02:44] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:02:44] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 23:02:44] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 23:02:45] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 23:02:45] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 23:02:45] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 23:02:45] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 23:02:45] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 23:02:45] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 23:02:45] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 23:02:46] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 23:03:02] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 23:03:03] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 23:03:03] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 23:03:03] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 23:03:03] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 23:03:03] d2.utils.env INFO: Using a generated random seed 7506273
[09/30 23:03:04] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:03:04] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:03:04] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 23:03:04] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 23:03:05] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 23:03:05] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 23:03:05] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 23:03:05] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 23:03:05] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 23:03:05] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 23:03:05] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 23:03:06] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 23:08:13] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 23:08:14] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 23:08:14] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 23:08:14] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 23:08:14] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 23:08:14] d2.utils.env INFO: Using a generated random seed 18469536
[09/30 23:08:15] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:08:15] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:08:15] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 23:08:15] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 23:08:16] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 23:08:16] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 23:08:16] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 23:08:16] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 23:08:16] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 23:08:16] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 23:08:16] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 23:08:17] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 23:09:45] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 23:09:46] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 23:09:46] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 23:09:46] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 23:09:46] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 23:09:46] d2.utils.env INFO: Using a generated random seed 50212234
[09/30 23:09:47] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:09:47] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:09:47] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 23:09:47] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 23:09:48] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 23:09:48] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 23:09:48] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 23:09:48] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 23:09:48] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 23:09:48] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 23:09:48] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 23:09:49] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[09/30 23:50:53] detectron2 INFO: Rank of current process: 0. World size: 1
[09/30 23:50:54] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[09/30 23:50:54] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[09/30 23:50:54] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[09/30 23:50:54] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[09/30 23:50:54] d2.utils.env INFO: Using a generated random seed 58529353
[09/30 23:50:55] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:50:55] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[09/30 23:50:55] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[09/30 23:50:55] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[09/30 23:50:56] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[09/30 23:50:56] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[09/30 23:50:56] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[09/30 23:50:56] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[09/30 23:50:56] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[09/30 23:50:56] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[09/30 23:50:56] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[09/30 23:50:57] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:14:30] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:14:30] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:14:30] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:14:30] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:14:30] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:14:30] d2.utils.env INFO: Using a generated random seed 34780907
[10/01 00:14:31] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:14:31] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:14:32] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:14:32] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:14:32] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:14:33] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:14:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:14:33] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:14:33] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:14:33] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:14:33] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:14:33] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:19:42] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:19:42] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:19:42] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:19:42] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:19:42] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:19:42] d2.utils.env INFO: Using a generated random seed 46774062
[10/01 00:19:42] d2.config.instantiate ERROR: Error when instantiating detectron2.modeling.backbone.hieradet.Hiera!
[10/01 00:21:28] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:21:29] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:21:29] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:21:29] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:21:29] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:21:29] d2.utils.env INFO: Using a generated random seed 33619312
[10/01 00:22:04] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:22:04] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:22:04] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:22:04] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:22:04] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:22:04] d2.utils.env INFO: Using a generated random seed 8961142
[10/01 00:22:05] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:22:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:22:06] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:22:06] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:22:06] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:22:07] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:22:07] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:22:07] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:22:07] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:22:07] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:22:07] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:22:07] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:23:15] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:23:16] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:23:16] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:23:16] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:23:16] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:23:16] d2.utils.env INFO: Using a generated random seed 20398499
[10/01 00:23:17] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:23:17] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:23:17] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:23:17] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:23:18] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:23:18] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:23:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:23:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:23:18] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:23:18] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:23:18] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:23:18] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:41:49] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:41:49] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:41:49] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:41:49] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:41:49] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:41:49] d2.utils.env INFO: Using a generated random seed 53771045
[10/01 00:41:50] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:41:50] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:41:51] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:41:51] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:41:51] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:41:52] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:41:52] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:41:52] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:41:52] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:41:52] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:41:52] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:41:52] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:48:01] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:48:02] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:48:02] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:48:02] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:48:02] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:48:02] d2.utils.env INFO: Using a generated random seed 6141899
[10/01 00:48:03] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:48:03] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:48:03] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:48:03] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:48:04] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:48:04] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:48:04] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:48:04] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:48:04] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:48:04] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:48:04] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:48:05] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:49:04] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:49:04] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:49:04] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:49:04] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:49:05] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:49:05] d2.utils.env INFO: Using a generated random seed 9056486
[10/01 00:49:05] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:49:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:49:06] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:49:06] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:49:06] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:49:07] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:49:07] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:49:07] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:49:07] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:49:07] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:49:07] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:49:08] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:50:09] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:50:10] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:50:10] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:50:10] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:50:10] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:50:10] d2.utils.env INFO: Using a generated random seed 14200470
[10/01 00:50:11] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:50:11] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:50:11] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:50:11] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:50:12] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:50:12] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:50:12] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:50:12] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:50:12] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:50:12] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:50:12] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:50:12] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:52:06] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:52:07] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:52:07] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:52:07] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:52:07] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:52:07] d2.utils.env INFO: Using a generated random seed 11314738
[10/01 00:52:08] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:52:08] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:52:08] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:52:08] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:52:09] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:52:09] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:52:09] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:52:09] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:52:09] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:52:09] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:52:09] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:52:09] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 00:52:56] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 00:52:56] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 00:52:56] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 00:52:56] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 00:52:56] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 00:52:56] d2.utils.env INFO: Using a generated random seed 60933081
[10/01 00:52:57] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:52:57] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 00:52:58] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 00:52:58] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 00:52:58] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 00:52:58] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 00:52:58] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 00:52:58] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 00:52:58] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 00:52:59] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 00:52:59] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 00:52:59] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 01:59:18] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 01:59:18] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 01:59:18] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 01:59:18] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 01:59:18] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 01:59:18] d2.utils.env INFO: Using a generated random seed 22957695
[10/01 02:00:04] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:00:05] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:00:05] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:00:05] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:00:05] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:00:05] d2.utils.env INFO: Using a generated random seed 9401733
[10/01 02:00:05] d2.config.instantiate ERROR: Error when instantiating detectron2.modeling.backbone.hieradet.Hiera!
[10/01 02:23:28] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:23:28] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:23:28] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:23:28] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:23:28] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:23:28] d2.utils.env INFO: Using a generated random seed 33028753
[10/01 02:23:29] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:23:29] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:23:30] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:23:30] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:23:31] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:23:31] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:23:31] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:23:31] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:23:31] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:23:31] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:23:31] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:23:32] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 02:26:56] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:26:57] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:26:57] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:26:57] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:26:57] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:26:57] d2.utils.env INFO: Using a generated random seed 61157091
[10/01 02:26:57] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:26:58] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:26:58] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:26:58] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:26:58] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:26:59] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:26:59] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:26:59] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:26:59] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:26:59] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:26:59] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:27:00] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 02:36:52] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:36:53] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:36:53] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:36:53] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:36:53] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:36:53] d2.utils.env INFO: Using a generated random seed 57147849
[10/01 02:36:54] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:36:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:36:54] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:36:54] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:36:54] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:36:55] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:36:55] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:36:55] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:36:55] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:36:55] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:36:55] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:36:55] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 02:45:47] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:45:47] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:45:47] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:45:47] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:45:47] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:45:47] d2.utils.env INFO: Using a generated random seed 51985266
[10/01 02:45:48] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:45:48] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:45:49] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:45:49] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:45:49] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:45:50] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:45:50] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:45:50] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:45:50] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:45:50] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:45:50] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:45:50] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 02:47:01] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:47:01] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:47:01] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:47:01] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:47:01] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:47:01] d2.utils.env INFO: Using a generated random seed 5777117
[10/01 02:47:02] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:47:02] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:47:02] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:47:02] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:47:03] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:47:03] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:47:03] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:47:03] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:47:03] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:47:04] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:47:04] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:47:05] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 02:49:34] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:49:34] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:49:34] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:49:34] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:49:34] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:49:34] d2.utils.env INFO: Using a generated random seed 38967531
[10/01 02:49:35] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:49:35] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:49:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:49:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:49:36] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:49:37] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:49:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:49:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:49:37] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:49:37] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:49:37] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:49:37] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 02:51:05] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 02:51:06] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 02:51:06] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 02:51:06] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["p2", "p3", "p4", "p5"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 02:51:06] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 02:51:06] d2.utils.env INFO: Using a generated random seed 10468521
[10/01 02:51:07] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:51:07] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 02:51:07] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 02:51:07] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 02:51:08] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 02:51:08] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 02:51:08] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 02:51:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 02:51:08] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 02:51:08] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 02:51:08] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 02:51:09] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 03:01:50] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 03:01:51] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 03:01:51] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 03:01:51] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 03:01:51] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 03:01:51] d2.utils.env INFO: Using a generated random seed 55357025
[10/01 03:01:52] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:01:52] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:01:52] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 03:01:52] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 03:01:53] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 03:01:53] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 03:01:53] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 03:01:53] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 03:01:53] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 03:01:53] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 03:01:53] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 03:01:54] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 03:09:08] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 03:09:09] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 03:09:09] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 03:09:09] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 03:09:09] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 03:09:09] d2.utils.env INFO: Using a generated random seed 13428195
[10/01 03:09:10] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:09:10] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:09:10] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 03:09:10] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 03:09:11] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 03:09:11] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 03:09:11] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 03:09:11] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 03:09:11] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 03:09:11] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 03:09:11] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 03:09:12] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 03:09:35] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 03:09:35] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 03:09:35] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 03:09:35] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 03:09:35] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 03:09:35] d2.utils.env INFO: Using a generated random seed 39819433
[10/01 03:09:36] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:09:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:09:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 03:09:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 03:09:37] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 03:09:37] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 03:09:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 03:09:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 03:09:37] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 03:09:37] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 03:09:38] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 03:09:38] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 03:15:37] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 03:15:38] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 03:15:38] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 03:15:38] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 03:15:38] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 03:15:38] d2.utils.env INFO: Using a generated random seed 42384166
[10/01 03:15:39] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:15:39] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:15:39] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 03:15:39] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 03:15:40] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 03:15:40] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 03:15:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 03:15:40] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 03:15:40] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 03:15:40] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 03:15:41] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 03:15:41] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 03:20:22] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 03:20:23] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 03:20:23] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 03:20:23] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 03:20:23] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 03:20:23] d2.utils.env INFO: Using a generated random seed 27354134
[10/01 03:20:24] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:20:24] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:20:24] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 03:20:24] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 03:20:25] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 03:20:25] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 03:20:25] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 03:20:25] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 03:20:25] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 03:20:25] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 03:20:25] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 03:20:25] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 03:22:40] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 03:22:41] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 03:22:41] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 03:22:41] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 03:22:41] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 03:22:41] d2.utils.env INFO: Using a generated random seed 45510237
[10/01 03:22:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:22:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 03:22:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 03:22:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 03:22:43] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 03:22:43] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 03:22:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 03:22:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 03:22:43] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 03:22:43] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 03:22:43] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 03:22:44] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 09:41:53] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 09:41:53] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 09:41:53] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 09:41:53] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 09:41:53] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 09:41:53] d2.utils.env INFO: Using a generated random seed 57951660
[10/01 09:41:54] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 09:41:54] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 09:41:56] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 09:41:56] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 09:41:56] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 09:41:57] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 09:41:57] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 09:41:57] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 09:41:57] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 09:41:57] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 09:41:57] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 09:41:57] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 09:44:20] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 09:44:20] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 09:44:20] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 09:44:20] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 09:44:20] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 09:44:20] d2.utils.env INFO: Using a generated random seed 24929750
[10/01 09:44:21] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 09:44:21] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 09:44:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 09:44:22] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 09:44:22] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 09:44:23] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 09:44:23] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 09:44:23] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 09:44:23] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 09:44:23] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 09:44:23] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 09:44:23] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 10:22:40] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 10:22:41] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 10:22:41] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 10:22:41] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 10:22:41] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 10:22:41] d2.utils.env INFO: Using a generated random seed 45227695
[10/01 10:22:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 10:22:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 10:22:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 10:22:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 10:22:43] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 10:22:43] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 10:22:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 10:22:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 10:22:43] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 10:22:43] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 10:22:43] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 10:22:44] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 10:24:30] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 10:24:30] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 10:24:30] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 10:24:30] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 10:24:30] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 10:24:30] d2.utils.env INFO: Using a generated random seed 34993401
[10/01 10:24:31] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 10:24:31] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 10:24:32] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 10:24:32] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 10:24:32] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 10:24:33] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 10:24:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 10:24:33] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 10:24:33] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 10:24:33] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 10:24:33] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 10:24:33] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 11:15:29] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 11:15:29] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 11:15:29] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 11:15:29] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 11:15:29] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 11:15:29] d2.utils.env INFO: Using a generated random seed 33718355
[10/01 11:15:30] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 11:15:30] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 11:15:31] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 11:15:31] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 11:15:32] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 11:15:32] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 11:15:32] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 11:15:32] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 11:15:32] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 11:15:32] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 11:15:32] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 11:15:33] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 14:23:05] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 14:23:05] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 14:23:05] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 14:23:05] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 14:23:05] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 14:23:05] d2.utils.env INFO: Using a generated random seed 9778787
[10/01 14:23:06] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 14:23:06] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 14:23:07] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.0.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.layers.1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 14:23:07] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 14:23:08] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 14:23:08] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 14:23:08] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 14:23:08] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 14:23:08] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 14:23:09] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 14:23:09] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 14:23:09] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 14:42:51] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 14:42:51] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 14:42:51] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 14:42:51] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 14:42:51] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 14:42:51] d2.utils.env INFO: Using a generated random seed 55990095
[10/01 14:45:13] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 14:45:14] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 14:45:14] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 14:45:14] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 14:45:14] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 14:45:14] d2.utils.env INFO: Using a generated random seed 18231441
[10/01 14:45:15] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 14:45:15] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 14:45:15] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 14:45:15] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 14:45:16] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 14:45:16] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 14:45:16] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 14:45:16] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 14:45:16] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 14:45:16] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 14:45:16] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 14:45:17] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 14:58:02] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 14:58:02] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 14:58:02] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 14:58:02] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 14:58:02] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 14:58:02] d2.utils.env INFO: Using a generated random seed 6878516
[10/01 14:58:03] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 14:58:03] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 14:58:04] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 14:58:04] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 14:58:04] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 14:58:05] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 14:58:05] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 14:58:05] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 14:58:05] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 14:58:05] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 14:58:05] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 14:58:05] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:26:46] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:26:46] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:26:46] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:26:46] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:26:46] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:26:46] d2.utils.env INFO: Using a generated random seed 50639042
[10/01 15:26:47] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:26:47] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:26:47] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:26:47] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:26:48] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:26:48] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:26:48] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:26:48] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:26:48] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:26:49] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:26:49] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:26:49] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:29:41] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:29:41] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:29:41] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:29:41] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:29:41] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:29:41] d2.utils.env INFO: Using a generated random seed 45777383
[10/01 15:29:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:29:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:29:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:29:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:29:43] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:29:43] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:29:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:29:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:29:43] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:29:43] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:29:44] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:29:44] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:33:03] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:33:04] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:33:04] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:33:04] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:33:04] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:33:04] d2.utils.env INFO: Using a generated random seed 8405112
[10/01 15:33:05] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:33:05] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:33:05] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:33:05] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:33:06] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:33:06] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:33:06] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:33:06] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:33:06] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:33:06] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:33:06] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:33:07] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:40:19] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:40:20] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:40:20] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:40:20] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:40:20] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:40:20] d2.utils.env INFO: Using a generated random seed 24427994
[10/01 15:40:21] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:40:21] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:40:21] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:40:21] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:40:22] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:40:22] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:40:22] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:40:22] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:40:22] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:40:22] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:40:22] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:40:23] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:45:25] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:45:25] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:45:25] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:45:25] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:45:25] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:45:25] d2.utils.env INFO: Using a generated random seed 29907384
[10/01 15:45:26] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:45:26] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:45:27] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:45:27] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:45:27] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:45:27] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:45:27] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:45:27] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:45:27] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:45:28] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:45:28] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:45:28] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:48:16] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:48:17] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:48:17] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:48:17] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:48:17] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:48:17] d2.utils.env INFO: Using a generated random seed 21153210
[10/01 15:48:18] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:48:18] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:48:18] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:48:18] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:48:18] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:48:19] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:48:19] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:48:19] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:48:19] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:48:19] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:48:19] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:48:19] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 15:48:35] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 15:48:35] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 15:48:35] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 15:48:35] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 15:48:35] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 15:48:35] d2.utils.env INFO: Using a generated random seed 39759239
[10/01 15:48:36] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:48:36] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 15:48:36] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 15:48:36] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 15:48:37] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 15:48:37] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 15:48:37] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 15:48:37] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 15:48:37] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 15:48:37] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 15:48:37] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 15:48:38] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:01:52] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:01:52] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:01:52] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:01:52] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:01:52] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:01:52] d2.utils.env INFO: Using a generated random seed 56759335
[10/01 16:01:53] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:01:53] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:01:53] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:01:53] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:01:54] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:01:54] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:01:54] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:01:54] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:01:54] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:01:54] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:01:54] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:01:55] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:09:40] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:09:40] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:09:40] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:09:40] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:09:40] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:09:40] d2.utils.env INFO: Using a generated random seed 44837467
[10/01 16:09:41] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:09:41] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:09:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:09:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:09:42] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:09:42] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:09:42] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:09:42] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:09:42] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:09:43] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:09:43] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:09:43] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:10:23] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:10:23] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:10:23] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:10:23] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:10:23] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:10:23] d2.utils.env INFO: Using a generated random seed 28064040
[10/01 16:10:24] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:10:24] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:10:25] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:10:25] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:10:25] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:10:26] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:10:26] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:10:26] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:10:26] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:10:26] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:10:26] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:10:26] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:13:56] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:13:56] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:13:56] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:13:56] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:13:56] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:13:56] d2.utils.env INFO: Using a generated random seed 60711683
[10/01 16:13:57] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:13:57] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:13:57] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:13:57] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:13:58] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:13:58] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:13:58] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:13:58] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:13:58] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:13:58] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:13:58] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:13:59] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:14:03] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0010 s/iter. Inference: 0.2489 s/iter. Eval: 0.0719 s/iter. Total: 0.3218 s/iter. ETA=0:26:45
[10/01 16:14:08] d2.evaluation.evaluator INFO: Inference done 29/5000. Dataloading: 0.0012 s/iter. Inference: 0.2503 s/iter. Eval: 0.0404 s/iter. Total: 0.2920 s/iter. ETA=0:24:11
[10/01 16:24:28] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:24:29] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:24:29] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:24:29] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:24:29] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:24:29] d2.utils.env INFO: Using a generated random seed 33335686
[10/01 16:24:30] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:24:30] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:24:30] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:24:30] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:24:31] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:24:31] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:24:31] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:24:31] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:24:31] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:24:31] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:24:31] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:24:31] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:24:35] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0010 s/iter. Inference: 0.2265 s/iter. Eval: 0.0001 s/iter. Total: 0.2276 s/iter. ETA=0:18:55
[10/01 16:24:40] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0013 s/iter. Inference: 0.2302 s/iter. Eval: 0.0001 s/iter. Total: 0.2316 s/iter. ETA=0:19:10
[10/01 16:24:45] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0013 s/iter. Inference: 0.2311 s/iter. Eval: 0.0001 s/iter. Total: 0.2326 s/iter. ETA=0:19:10
[10/01 16:24:50] d2.evaluation.evaluator INFO: Inference done 77/5000. Dataloading: 0.0013 s/iter. Inference: 0.2321 s/iter. Eval: 0.0001 s/iter. Total: 0.2336 s/iter. ETA=0:19:09
[10/01 16:24:55] d2.evaluation.evaluator INFO: Inference done 100/5000. Dataloading: 0.0013 s/iter. Inference: 0.2302 s/iter. Eval: 0.0001 s/iter. Total: 0.2317 s/iter. ETA=0:18:55
[10/01 16:25:01] d2.evaluation.evaluator INFO: Inference done 122/5000. Dataloading: 0.0013 s/iter. Inference: 0.2300 s/iter. Eval: 0.0001 s/iter. Total: 0.2315 s/iter. ETA=0:18:49
[10/01 16:25:06] d2.evaluation.evaluator INFO: Inference done 143/5000. Dataloading: 0.0013 s/iter. Inference: 0.2317 s/iter. Eval: 0.0001 s/iter. Total: 0.2332 s/iter. ETA=0:18:52
[10/01 16:25:11] d2.evaluation.evaluator INFO: Inference done 165/5000. Dataloading: 0.0013 s/iter. Inference: 0.2318 s/iter. Eval: 0.0001 s/iter. Total: 0.2332 s/iter. ETA=0:18:47
[10/01 16:25:16] d2.evaluation.evaluator INFO: Inference done 187/5000. Dataloading: 0.0013 s/iter. Inference: 0.2316 s/iter. Eval: 0.0001 s/iter. Total: 0.2331 s/iter. ETA=0:18:41
[10/01 16:25:21] d2.evaluation.evaluator INFO: Inference done 208/5000. Dataloading: 0.0013 s/iter. Inference: 0.2326 s/iter. Eval: 0.0001 s/iter. Total: 0.2341 s/iter. ETA=0:18:41
[10/01 16:25:26] d2.evaluation.evaluator INFO: Inference done 230/5000. Dataloading: 0.0013 s/iter. Inference: 0.2321 s/iter. Eval: 0.0001 s/iter. Total: 0.2336 s/iter. ETA=0:18:34
[10/01 16:25:31] d2.evaluation.evaluator INFO: Inference done 251/5000. Dataloading: 0.0013 s/iter. Inference: 0.2326 s/iter. Eval: 0.0001 s/iter. Total: 0.2340 s/iter. ETA=0:18:31
[10/01 16:25:36] d2.evaluation.evaluator INFO: Inference done 273/5000. Dataloading: 0.0013 s/iter. Inference: 0.2321 s/iter. Eval: 0.0001 s/iter. Total: 0.2336 s/iter. ETA=0:18:24
[10/01 16:25:41] d2.evaluation.evaluator INFO: Inference done 294/5000. Dataloading: 0.0013 s/iter. Inference: 0.2326 s/iter. Eval: 0.0001 s/iter. Total: 0.2341 s/iter. ETA=0:18:21
[10/01 16:25:46] d2.evaluation.evaluator INFO: Inference done 316/5000. Dataloading: 0.0013 s/iter. Inference: 0.2325 s/iter. Eval: 0.0001 s/iter. Total: 0.2340 s/iter. ETA=0:18:15
[10/01 16:25:51] d2.evaluation.evaluator INFO: Inference done 338/5000. Dataloading: 0.0013 s/iter. Inference: 0.2327 s/iter. Eval: 0.0001 s/iter. Total: 0.2342 s/iter. ETA=0:18:11
[10/01 16:25:56] d2.evaluation.evaluator INFO: Inference done 359/5000. Dataloading: 0.0013 s/iter. Inference: 0.2330 s/iter. Eval: 0.0001 s/iter. Total: 0.2345 s/iter. ETA=0:18:08
[10/01 16:26:02] d2.evaluation.evaluator INFO: Inference done 381/5000. Dataloading: 0.0013 s/iter. Inference: 0.2329 s/iter. Eval: 0.0001 s/iter. Total: 0.2344 s/iter. ETA=0:18:02
[10/01 16:26:07] d2.evaluation.evaluator INFO: Inference done 403/5000. Dataloading: 0.0013 s/iter. Inference: 0.2327 s/iter. Eval: 0.0001 s/iter. Total: 0.2342 s/iter. ETA=0:17:56
[10/01 16:26:12] d2.evaluation.evaluator INFO: Inference done 425/5000. Dataloading: 0.0013 s/iter. Inference: 0.2325 s/iter. Eval: 0.0001 s/iter. Total: 0.2340 s/iter. ETA=0:17:50
[10/01 16:26:17] d2.evaluation.evaluator INFO: Inference done 446/5000. Dataloading: 0.0013 s/iter. Inference: 0.2329 s/iter. Eval: 0.0001 s/iter. Total: 0.2344 s/iter. ETA=0:17:47
[10/01 16:26:22] d2.evaluation.evaluator INFO: Inference done 468/5000. Dataloading: 0.0013 s/iter. Inference: 0.2330 s/iter. Eval: 0.0001 s/iter. Total: 0.2345 s/iter. ETA=0:17:42
[10/01 16:26:27] d2.evaluation.evaluator INFO: Inference done 489/5000. Dataloading: 0.0013 s/iter. Inference: 0.2333 s/iter. Eval: 0.0001 s/iter. Total: 0.2348 s/iter. ETA=0:17:39
[10/01 16:28:31] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:28:31] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:28:31] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:28:31] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:28:31] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:28:31] d2.utils.env INFO: Using a generated random seed 35924773
[10/01 16:28:32] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:28:32] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:28:33] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:28:33] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:28:33] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:28:33] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:28:33] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:28:33] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:28:33] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:28:34] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:28:34] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:28:34] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:28:37] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0008 s/iter. Inference: 0.2287 s/iter. Eval: 0.0001 s/iter. Total: 0.2296 s/iter. ETA=0:19:05
[10/01 16:28:43] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0011 s/iter. Inference: 0.2321 s/iter. Eval: 0.0001 s/iter. Total: 0.2333 s/iter. ETA=0:19:18
[10/01 16:28:48] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0012 s/iter. Inference: 0.2329 s/iter. Eval: 0.0001 s/iter. Total: 0.2342 s/iter. ETA=0:19:18
[10/01 16:28:53] d2.evaluation.evaluator INFO: Inference done 76/5000. Dataloading: 0.0012 s/iter. Inference: 0.2347 s/iter. Eval: 0.0001 s/iter. Total: 0.2360 s/iter. ETA=0:19:21
[10/01 16:28:58] d2.evaluation.evaluator INFO: Inference done 98/5000. Dataloading: 0.0012 s/iter. Inference: 0.2331 s/iter. Eval: 0.0001 s/iter. Total: 0.2344 s/iter. ETA=0:19:08
[10/01 16:29:03] d2.evaluation.evaluator INFO: Inference done 120/5000. Dataloading: 0.0012 s/iter. Inference: 0.2326 s/iter. Eval: 0.0001 s/iter. Total: 0.2340 s/iter. ETA=0:19:01
[10/01 16:29:08] d2.evaluation.evaluator INFO: Inference done 141/5000. Dataloading: 0.0012 s/iter. Inference: 0.2340 s/iter. Eval: 0.0001 s/iter. Total: 0.2354 s/iter. ETA=0:19:03
[10/01 16:29:13] d2.evaluation.evaluator INFO: Inference done 162/5000. Dataloading: 0.0012 s/iter. Inference: 0.2353 s/iter. Eval: 0.0001 s/iter. Total: 0.2367 s/iter. ETA=0:19:05
[10/01 16:29:18] d2.evaluation.evaluator INFO: Inference done 183/5000. Dataloading: 0.0012 s/iter. Inference: 0.2357 s/iter. Eval: 0.0001 s/iter. Total: 0.2370 s/iter. ETA=0:19:01
[10/01 16:29:23] d2.evaluation.evaluator INFO: Inference done 204/5000. Dataloading: 0.0012 s/iter. Inference: 0.2364 s/iter. Eval: 0.0001 s/iter. Total: 0.2378 s/iter. ETA=0:19:00
[10/01 16:29:29] d2.evaluation.evaluator INFO: Inference done 225/5000. Dataloading: 0.0013 s/iter. Inference: 0.2374 s/iter. Eval: 0.0001 s/iter. Total: 0.2388 s/iter. ETA=0:19:00
[10/01 16:29:34] d2.evaluation.evaluator INFO: Inference done 246/5000. Dataloading: 0.0012 s/iter. Inference: 0.2383 s/iter. Eval: 0.0001 s/iter. Total: 0.2397 s/iter. ETA=0:18:59
[10/01 16:29:39] d2.evaluation.evaluator INFO: Inference done 267/5000. Dataloading: 0.0013 s/iter. Inference: 0.2387 s/iter. Eval: 0.0001 s/iter. Total: 0.2401 s/iter. ETA=0:18:56
[10/01 16:29:44] d2.evaluation.evaluator INFO: Inference done 288/5000. Dataloading: 0.0013 s/iter. Inference: 0.2390 s/iter. Eval: 0.0001 s/iter. Total: 0.2403 s/iter. ETA=0:18:52
[10/01 16:29:49] d2.evaluation.evaluator INFO: Inference done 309/5000. Dataloading: 0.0013 s/iter. Inference: 0.2393 s/iter. Eval: 0.0001 s/iter. Total: 0.2407 s/iter. ETA=0:18:49
[10/01 16:29:54] d2.evaluation.evaluator INFO: Inference done 330/5000. Dataloading: 0.0013 s/iter. Inference: 0.2397 s/iter. Eval: 0.0001 s/iter. Total: 0.2411 s/iter. ETA=0:18:45
[10/01 16:29:59] d2.evaluation.evaluator INFO: Inference done 350/5000. Dataloading: 0.0013 s/iter. Inference: 0.2402 s/iter. Eval: 0.0001 s/iter. Total: 0.2416 s/iter. ETA=0:18:43
[10/01 16:30:05] d2.evaluation.evaluator INFO: Inference done 371/5000. Dataloading: 0.0013 s/iter. Inference: 0.2406 s/iter. Eval: 0.0001 s/iter. Total: 0.2420 s/iter. ETA=0:18:40
[10/01 16:30:10] d2.evaluation.evaluator INFO: Inference done 393/5000. Dataloading: 0.0013 s/iter. Inference: 0.2403 s/iter. Eval: 0.0001 s/iter. Total: 0.2417 s/iter. ETA=0:18:33
[10/01 16:30:15] d2.evaluation.evaluator INFO: Inference done 414/5000. Dataloading: 0.0013 s/iter. Inference: 0.2406 s/iter. Eval: 0.0001 s/iter. Total: 0.2420 s/iter. ETA=0:18:29
[10/01 16:30:20] d2.evaluation.evaluator INFO: Inference done 435/5000. Dataloading: 0.0013 s/iter. Inference: 0.2406 s/iter. Eval: 0.0001 s/iter. Total: 0.2420 s/iter. ETA=0:18:24
[10/01 16:30:25] d2.evaluation.evaluator INFO: Inference done 456/5000. Dataloading: 0.0013 s/iter. Inference: 0.2407 s/iter. Eval: 0.0001 s/iter. Total: 0.2421 s/iter. ETA=0:18:20
[10/01 16:30:30] d2.evaluation.evaluator INFO: Inference done 476/5000. Dataloading: 0.0013 s/iter. Inference: 0.2413 s/iter. Eval: 0.0001 s/iter. Total: 0.2427 s/iter. ETA=0:18:17
[10/01 16:30:35] d2.evaluation.evaluator INFO: Inference done 496/5000. Dataloading: 0.0013 s/iter. Inference: 0.2416 s/iter. Eval: 0.0001 s/iter. Total: 0.2430 s/iter. ETA=0:18:14
[10/01 16:34:21] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 16:34:21] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 16:34:21] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 16:34:21] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 16:34:21] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 16:34:21] d2.utils.env INFO: Using a generated random seed 25746224
[10/01 16:34:22] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:34:22] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 16:34:22] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 16:34:22] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 16:34:23] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 16:34:23] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 16:34:23] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 16:34:23] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 16:34:23] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 16:34:23] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 16:34:23] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 16:34:24] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 16:34:27] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0008 s/iter. Inference: 0.2282 s/iter. Eval: 0.0001 s/iter. Total: 0.2290 s/iter. ETA=0:19:02
[10/01 16:34:32] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0012 s/iter. Inference: 0.2320 s/iter. Eval: 0.0001 s/iter. Total: 0.2333 s/iter. ETA=0:19:18
[10/01 16:34:38] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0012 s/iter. Inference: 0.2327 s/iter. Eval: 0.0001 s/iter. Total: 0.2341 s/iter. ETA=0:19:17
[10/01 16:34:43] d2.evaluation.evaluator INFO: Inference done 76/5000. Dataloading: 0.0013 s/iter. Inference: 0.2341 s/iter. Eval: 0.0001 s/iter. Total: 0.2355 s/iter. ETA=0:19:19
[10/01 16:34:48] d2.evaluation.evaluator INFO: Inference done 98/5000. Dataloading: 0.0013 s/iter. Inference: 0.2326 s/iter. Eval: 0.0001 s/iter. Total: 0.2340 s/iter. ETA=0:19:06
[10/01 16:34:53] d2.evaluation.evaluator INFO: Inference done 120/5000. Dataloading: 0.0012 s/iter. Inference: 0.2330 s/iter. Eval: 0.0001 s/iter. Total: 0.2344 s/iter. ETA=0:19:03
[10/01 16:34:58] d2.evaluation.evaluator INFO: Inference done 141/5000. Dataloading: 0.0013 s/iter. Inference: 0.2354 s/iter. Eval: 0.0001 s/iter. Total: 0.2368 s/iter. ETA=0:19:10
[10/01 16:35:03] d2.evaluation.evaluator INFO: Inference done 161/5000. Dataloading: 0.0013 s/iter. Inference: 0.2371 s/iter. Eval: 0.0001 s/iter. Total: 0.2385 s/iter. ETA=0:19:14
[10/01 16:35:08] d2.evaluation.evaluator INFO: Inference done 182/5000. Dataloading: 0.0013 s/iter. Inference: 0.2371 s/iter. Eval: 0.0001 s/iter. Total: 0.2385 s/iter. ETA=0:19:09
[10/01 16:35:13] d2.evaluation.evaluator INFO: Inference done 203/5000. Dataloading: 0.0013 s/iter. Inference: 0.2376 s/iter. Eval: 0.0001 s/iter. Total: 0.2390 s/iter. ETA=0:19:06
[10/01 16:35:18] d2.evaluation.evaluator INFO: Inference done 224/5000. Dataloading: 0.0013 s/iter. Inference: 0.2385 s/iter. Eval: 0.0001 s/iter. Total: 0.2399 s/iter. ETA=0:19:05
[10/01 16:35:23] d2.evaluation.evaluator INFO: Inference done 244/5000. Dataloading: 0.0013 s/iter. Inference: 0.2394 s/iter. Eval: 0.0001 s/iter. Total: 0.2408 s/iter. ETA=0:19:05
[10/01 16:35:29] d2.evaluation.evaluator INFO: Inference done 265/5000. Dataloading: 0.0013 s/iter. Inference: 0.2396 s/iter. Eval: 0.0001 s/iter. Total: 0.2410 s/iter. ETA=0:19:01
[10/01 16:35:34] d2.evaluation.evaluator INFO: Inference done 286/5000. Dataloading: 0.0013 s/iter. Inference: 0.2397 s/iter. Eval: 0.0001 s/iter. Total: 0.2411 s/iter. ETA=0:18:56
[10/01 16:35:39] d2.evaluation.evaluator INFO: Inference done 307/5000. Dataloading: 0.0013 s/iter. Inference: 0.2402 s/iter. Eval: 0.0001 s/iter. Total: 0.2415 s/iter. ETA=0:18:53
[10/01 16:35:44] d2.evaluation.evaluator INFO: Inference done 328/5000. Dataloading: 0.0013 s/iter. Inference: 0.2404 s/iter. Eval: 0.0001 s/iter. Total: 0.2418 s/iter. ETA=0:18:49
[10/01 16:35:49] d2.evaluation.evaluator INFO: Inference done 349/5000. Dataloading: 0.0013 s/iter. Inference: 0.2409 s/iter. Eval: 0.0001 s/iter. Total: 0.2422 s/iter. ETA=0:18:46
[10/01 16:35:54] d2.evaluation.evaluator INFO: Inference done 370/5000. Dataloading: 0.0013 s/iter. Inference: 0.2412 s/iter. Eval: 0.0001 s/iter. Total: 0.2426 s/iter. ETA=0:18:43
[10/01 16:36:00] d2.evaluation.evaluator INFO: Inference done 392/5000. Dataloading: 0.0013 s/iter. Inference: 0.2410 s/iter. Eval: 0.0001 s/iter. Total: 0.2423 s/iter. ETA=0:18:36
[10/01 16:36:05] d2.evaluation.evaluator INFO: Inference done 413/5000. Dataloading: 0.0013 s/iter. Inference: 0.2411 s/iter. Eval: 0.0001 s/iter. Total: 0.2424 s/iter. ETA=0:18:32
[10/01 16:36:10] d2.evaluation.evaluator INFO: Inference done 434/5000. Dataloading: 0.0013 s/iter. Inference: 0.2412 s/iter. Eval: 0.0001 s/iter. Total: 0.2426 s/iter. ETA=0:18:27
[10/01 16:36:15] d2.evaluation.evaluator INFO: Inference done 455/5000. Dataloading: 0.0013 s/iter. Inference: 0.2412 s/iter. Eval: 0.0001 s/iter. Total: 0.2426 s/iter. ETA=0:18:22
[10/01 16:36:20] d2.evaluation.evaluator INFO: Inference done 475/5000. Dataloading: 0.0013 s/iter. Inference: 0.2419 s/iter. Eval: 0.0001 s/iter. Total: 0.2432 s/iter. ETA=0:18:20
[10/01 16:36:25] d2.evaluation.evaluator INFO: Inference done 495/5000. Dataloading: 0.0013 s/iter. Inference: 0.2422 s/iter. Eval: 0.0001 s/iter. Total: 0.2435 s/iter. ETA=0:18:17
[10/01 16:36:30] d2.evaluation.evaluator INFO: Inference done 517/5000. Dataloading: 0.0013 s/iter. Inference: 0.2416 s/iter. Eval: 0.0001 s/iter. Total: 0.2430 s/iter. ETA=0:18:09
[10/01 16:36:36] d2.evaluation.evaluator INFO: Inference done 538/5000. Dataloading: 0.0013 s/iter. Inference: 0.2419 s/iter. Eval: 0.0001 s/iter. Total: 0.2433 s/iter. ETA=0:18:05
[10/01 17:06:41] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:06:41] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:06:41] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:06:41] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:06:41] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:06:41] d2.utils.env INFO: Using a generated random seed 45929950
[10/01 17:07:22] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:07:23] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:07:23] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:07:23] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:07:23] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:07:23] d2.utils.env INFO: Using a generated random seed 27565155
[10/01 17:07:24] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:07:24] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:07:24] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:07:24] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:07:25] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:07:25] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:07:25] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:07:25] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:07:25] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:07:26] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:07:26] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:07:27] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:07:30] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0007 s/iter. Inference: 0.2314 s/iter. Eval: 0.0001 s/iter. Total: 0.2322 s/iter. ETA=0:19:18
[10/01 17:07:36] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0011 s/iter. Inference: 0.2351 s/iter. Eval: 0.0001 s/iter. Total: 0.2363 s/iter. ETA=0:19:33
[10/01 17:07:41] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0013 s/iter. Inference: 0.2349 s/iter. Eval: 0.0001 s/iter. Total: 0.2362 s/iter. ETA=0:19:28
[10/01 17:07:46] d2.evaluation.evaluator INFO: Inference done 76/5000. Dataloading: 0.0013 s/iter. Inference: 0.2363 s/iter. Eval: 0.0001 s/iter. Total: 0.2377 s/iter. ETA=0:19:30
[10/01 17:07:51] d2.evaluation.evaluator INFO: Inference done 98/5000. Dataloading: 0.0012 s/iter. Inference: 0.2348 s/iter. Eval: 0.0001 s/iter. Total: 0.2362 s/iter. ETA=0:19:17
[10/01 17:07:56] d2.evaluation.evaluator INFO: Inference done 120/5000. Dataloading: 0.0012 s/iter. Inference: 0.2343 s/iter. Eval: 0.0001 s/iter. Total: 0.2356 s/iter. ETA=0:19:09
[10/01 17:08:01] d2.evaluation.evaluator INFO: Inference done 141/5000. Dataloading: 0.0012 s/iter. Inference: 0.2361 s/iter. Eval: 0.0001 s/iter. Total: 0.2375 s/iter. ETA=0:19:13
[10/01 17:08:06] d2.evaluation.evaluator INFO: Inference done 162/5000. Dataloading: 0.0012 s/iter. Inference: 0.2372 s/iter. Eval: 0.0001 s/iter. Total: 0.2386 s/iter. ETA=0:19:14
[10/01 17:08:12] d2.evaluation.evaluator INFO: Inference done 184/5000. Dataloading: 0.0012 s/iter. Inference: 0.2370 s/iter. Eval: 0.0001 s/iter. Total: 0.2383 s/iter. ETA=0:19:07
[10/01 17:08:17] d2.evaluation.evaluator INFO: Inference done 205/5000. Dataloading: 0.0012 s/iter. Inference: 0.2377 s/iter. Eval: 0.0001 s/iter. Total: 0.2390 s/iter. ETA=0:19:06
[10/01 17:08:22] d2.evaluation.evaluator INFO: Inference done 226/5000. Dataloading: 0.0012 s/iter. Inference: 0.2384 s/iter. Eval: 0.0001 s/iter. Total: 0.2398 s/iter. ETA=0:19:04
[10/01 17:08:27] d2.evaluation.evaluator INFO: Inference done 246/5000. Dataloading: 0.0012 s/iter. Inference: 0.2393 s/iter. Eval: 0.0001 s/iter. Total: 0.2407 s/iter. ETA=0:19:04
[10/01 17:08:32] d2.evaluation.evaluator INFO: Inference done 267/5000. Dataloading: 0.0012 s/iter. Inference: 0.2396 s/iter. Eval: 0.0001 s/iter. Total: 0.2409 s/iter. ETA=0:19:00
[10/01 17:08:37] d2.evaluation.evaluator INFO: Inference done 288/5000. Dataloading: 0.0012 s/iter. Inference: 0.2398 s/iter. Eval: 0.0001 s/iter. Total: 0.2412 s/iter. ETA=0:18:56
[10/01 17:08:42] d2.evaluation.evaluator INFO: Inference done 309/5000. Dataloading: 0.0012 s/iter. Inference: 0.2401 s/iter. Eval: 0.0001 s/iter. Total: 0.2414 s/iter. ETA=0:18:52
[10/01 17:08:47] d2.evaluation.evaluator INFO: Inference done 330/5000. Dataloading: 0.0012 s/iter. Inference: 0.2404 s/iter. Eval: 0.0001 s/iter. Total: 0.2417 s/iter. ETA=0:18:48
[10/01 17:08:53] d2.evaluation.evaluator INFO: Inference done 351/5000. Dataloading: 0.0012 s/iter. Inference: 0.2409 s/iter. Eval: 0.0001 s/iter. Total: 0.2422 s/iter. ETA=0:18:46
[10/01 17:08:58] d2.evaluation.evaluator INFO: Inference done 371/5000. Dataloading: 0.0012 s/iter. Inference: 0.2414 s/iter. Eval: 0.0001 s/iter. Total: 0.2427 s/iter. ETA=0:18:43
[10/01 17:09:03] d2.evaluation.evaluator INFO: Inference done 393/5000. Dataloading: 0.0012 s/iter. Inference: 0.2410 s/iter. Eval: 0.0001 s/iter. Total: 0.2423 s/iter. ETA=0:18:36
[10/01 17:09:08] d2.evaluation.evaluator INFO: Inference done 414/5000. Dataloading: 0.0012 s/iter. Inference: 0.2412 s/iter. Eval: 0.0001 s/iter. Total: 0.2426 s/iter. ETA=0:18:32
[10/01 17:09:13] d2.evaluation.evaluator INFO: Inference done 435/5000. Dataloading: 0.0012 s/iter. Inference: 0.2413 s/iter. Eval: 0.0001 s/iter. Total: 0.2427 s/iter. ETA=0:18:27
[10/01 17:09:18] d2.evaluation.evaluator INFO: Inference done 456/5000. Dataloading: 0.0012 s/iter. Inference: 0.2415 s/iter. Eval: 0.0001 s/iter. Total: 0.2428 s/iter. ETA=0:18:23
[10/01 17:09:24] d2.evaluation.evaluator INFO: Inference done 476/5000. Dataloading: 0.0012 s/iter. Inference: 0.2420 s/iter. Eval: 0.0001 s/iter. Total: 0.2434 s/iter. ETA=0:18:20
[10/01 17:09:29] d2.evaluation.evaluator INFO: Inference done 497/5000. Dataloading: 0.0012 s/iter. Inference: 0.2422 s/iter. Eval: 0.0001 s/iter. Total: 0.2435 s/iter. ETA=0:18:16
[10/01 17:09:34] d2.evaluation.evaluator INFO: Inference done 519/5000. Dataloading: 0.0012 s/iter. Inference: 0.2418 s/iter. Eval: 0.0001 s/iter. Total: 0.2432 s/iter. ETA=0:18:09
[10/01 17:09:39] d2.evaluation.evaluator INFO: Inference done 540/5000. Dataloading: 0.0012 s/iter. Inference: 0.2420 s/iter. Eval: 0.0001 s/iter. Total: 0.2434 s/iter. ETA=0:18:05
[10/01 17:11:40] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:11:41] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:11:41] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:11:41] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:11:41] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:11:41] d2.utils.env INFO: Using a generated random seed 45239145
[10/01 17:11:42] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:11:42] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:11:42] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:11:42] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:11:43] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:11:43] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:11:43] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:11:43] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:11:43] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:11:43] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:11:43] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:11:43] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:11:47] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0009 s/iter. Inference: 0.2306 s/iter. Eval: 0.0002 s/iter. Total: 0.2317 s/iter. ETA=0:19:15
[10/01 17:11:52] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0012 s/iter. Inference: 0.2363 s/iter. Eval: 0.0001 s/iter. Total: 0.2377 s/iter. ETA=0:19:40
[10/01 17:11:57] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0012 s/iter. Inference: 0.2340 s/iter. Eval: 0.0001 s/iter. Total: 0.2353 s/iter. ETA=0:19:23
[10/01 17:12:02] d2.evaluation.evaluator INFO: Inference done 76/5000. Dataloading: 0.0012 s/iter. Inference: 0.2357 s/iter. Eval: 0.0001 s/iter. Total: 0.2370 s/iter. ETA=0:19:27
[10/01 17:12:07] d2.evaluation.evaluator INFO: Inference done 98/5000. Dataloading: 0.0012 s/iter. Inference: 0.2340 s/iter. Eval: 0.0001 s/iter. Total: 0.2353 s/iter. ETA=0:19:13
[10/01 17:12:12] d2.evaluation.evaluator INFO: Inference done 120/5000. Dataloading: 0.0012 s/iter. Inference: 0.2334 s/iter. Eval: 0.0001 s/iter. Total: 0.2347 s/iter. ETA=0:19:05
[10/01 17:12:18] d2.evaluation.evaluator INFO: Inference done 141/5000. Dataloading: 0.0012 s/iter. Inference: 0.2349 s/iter. Eval: 0.0001 s/iter. Total: 0.2362 s/iter. ETA=0:19:07
[10/01 17:12:23] d2.evaluation.evaluator INFO: Inference done 162/5000. Dataloading: 0.0012 s/iter. Inference: 0.2362 s/iter. Eval: 0.0001 s/iter. Total: 0.2376 s/iter. ETA=0:19:09
[10/01 17:12:28] d2.evaluation.evaluator INFO: Inference done 183/5000. Dataloading: 0.0012 s/iter. Inference: 0.2365 s/iter. Eval: 0.0001 s/iter. Total: 0.2379 s/iter. ETA=0:19:05
[10/01 17:12:33] d2.evaluation.evaluator INFO: Inference done 203/5000. Dataloading: 0.0012 s/iter. Inference: 0.2380 s/iter. Eval: 0.0001 s/iter. Total: 0.2393 s/iter. ETA=0:19:08
[10/01 17:12:38] d2.evaluation.evaluator INFO: Inference done 224/5000. Dataloading: 0.0012 s/iter. Inference: 0.2380 s/iter. Eval: 0.0001 s/iter. Total: 0.2394 s/iter. ETA=0:19:03
[10/01 17:12:43] d2.evaluation.evaluator INFO: Inference done 244/5000. Dataloading: 0.0012 s/iter. Inference: 0.2390 s/iter. Eval: 0.0001 s/iter. Total: 0.2404 s/iter. ETA=0:19:03
[10/01 17:12:48] d2.evaluation.evaluator INFO: Inference done 265/5000. Dataloading: 0.0012 s/iter. Inference: 0.2394 s/iter. Eval: 0.0001 s/iter. Total: 0.2407 s/iter. ETA=0:18:59
[10/01 17:12:53] d2.evaluation.evaluator INFO: Inference done 286/5000. Dataloading: 0.0012 s/iter. Inference: 0.2396 s/iter. Eval: 0.0001 s/iter. Total: 0.2410 s/iter. ETA=0:18:56
[10/01 17:12:58] d2.evaluation.evaluator INFO: Inference done 307/5000. Dataloading: 0.0012 s/iter. Inference: 0.2401 s/iter. Eval: 0.0001 s/iter. Total: 0.2415 s/iter. ETA=0:18:53
[10/01 17:13:04] d2.evaluation.evaluator INFO: Inference done 328/5000. Dataloading: 0.0012 s/iter. Inference: 0.2403 s/iter. Eval: 0.0001 s/iter. Total: 0.2417 s/iter. ETA=0:18:49
[10/01 17:13:09] d2.evaluation.evaluator INFO: Inference done 348/5000. Dataloading: 0.0012 s/iter. Inference: 0.2408 s/iter. Eval: 0.0001 s/iter. Total: 0.2422 s/iter. ETA=0:18:46
[10/01 17:13:14] d2.evaluation.evaluator INFO: Inference done 369/5000. Dataloading: 0.0012 s/iter. Inference: 0.2412 s/iter. Eval: 0.0001 s/iter. Total: 0.2426 s/iter. ETA=0:18:43
[10/01 17:13:19] d2.evaluation.evaluator INFO: Inference done 390/5000. Dataloading: 0.0012 s/iter. Inference: 0.2410 s/iter. Eval: 0.0001 s/iter. Total: 0.2424 s/iter. ETA=0:18:37
[10/01 17:13:24] d2.evaluation.evaluator INFO: Inference done 411/5000. Dataloading: 0.0012 s/iter. Inference: 0.2413 s/iter. Eval: 0.0001 s/iter. Total: 0.2427 s/iter. ETA=0:18:33
[10/01 17:13:29] d2.evaluation.evaluator INFO: Inference done 432/5000. Dataloading: 0.0012 s/iter. Inference: 0.2415 s/iter. Eval: 0.0001 s/iter. Total: 0.2429 s/iter. ETA=0:18:29
[10/01 17:13:34] d2.evaluation.evaluator INFO: Inference done 453/5000. Dataloading: 0.0012 s/iter. Inference: 0.2416 s/iter. Eval: 0.0001 s/iter. Total: 0.2430 s/iter. ETA=0:18:24
[10/01 17:13:39] d2.evaluation.evaluator INFO: Inference done 473/5000. Dataloading: 0.0012 s/iter. Inference: 0.2422 s/iter. Eval: 0.0001 s/iter. Total: 0.2437 s/iter. ETA=0:18:23
[10/01 17:13:45] d2.evaluation.evaluator INFO: Inference done 493/5000. Dataloading: 0.0012 s/iter. Inference: 0.2426 s/iter. Eval: 0.0001 s/iter. Total: 0.2440 s/iter. ETA=0:18:19
[10/01 17:13:50] d2.evaluation.evaluator INFO: Inference done 515/5000. Dataloading: 0.0012 s/iter. Inference: 0.2422 s/iter. Eval: 0.0001 s/iter. Total: 0.2436 s/iter. ETA=0:18:12
[10/01 17:13:55] d2.evaluation.evaluator INFO: Inference done 536/5000. Dataloading: 0.0012 s/iter. Inference: 0.2423 s/iter. Eval: 0.0001 s/iter. Total: 0.2437 s/iter. ETA=0:18:07
[10/01 17:14:00] d2.evaluation.evaluator INFO: Inference done 557/5000. Dataloading: 0.0012 s/iter. Inference: 0.2423 s/iter. Eval: 0.0001 s/iter. Total: 0.2437 s/iter. ETA=0:18:02
[10/01 17:14:18] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:14:18] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:14:18] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:14:18] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:14:18] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:14:18] d2.utils.env INFO: Using a generated random seed 22940215
[10/01 17:14:19] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:14:19] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:14:20] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:14:20] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:14:20] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:14:20] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:14:20] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:14:20] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:14:20] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:14:21] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:14:21] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:14:21] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:14:25] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0007 s/iter. Inference: 0.2313 s/iter. Eval: 0.0001 s/iter. Total: 0.2321 s/iter. ETA=0:19:17
[10/01 17:14:30] d2.evaluation.evaluator INFO: Inference done 32/5000. Dataloading: 0.0012 s/iter. Inference: 0.2409 s/iter. Eval: 0.0004 s/iter. Total: 0.2425 s/iter. ETA=0:20:04
[10/01 17:14:35] d2.evaluation.evaluator INFO: Inference done 53/5000. Dataloading: 0.0012 s/iter. Inference: 0.2422 s/iter. Eval: 0.0007 s/iter. Total: 0.2441 s/iter. ETA=0:20:07
[10/01 17:14:40] d2.evaluation.evaluator INFO: Inference done 73/5000. Dataloading: 0.0012 s/iter. Inference: 0.2474 s/iter. Eval: 0.0006 s/iter. Total: 0.2493 s/iter. ETA=0:20:28
[10/01 17:14:45] d2.evaluation.evaluator INFO: Inference done 94/5000. Dataloading: 0.0012 s/iter. Inference: 0.2460 s/iter. Eval: 0.0010 s/iter. Total: 0.2482 s/iter. ETA=0:20:17
[10/01 17:14:50] d2.evaluation.evaluator INFO: Inference done 115/5000. Dataloading: 0.0012 s/iter. Inference: 0.2444 s/iter. Eval: 0.0009 s/iter. Total: 0.2466 s/iter. ETA=0:20:04
[10/01 17:14:55] d2.evaluation.evaluator INFO: Inference done 135/5000. Dataloading: 0.0012 s/iter. Inference: 0.2454 s/iter. Eval: 0.0008 s/iter. Total: 0.2475 s/iter. ETA=0:20:04
[10/01 17:15:01] d2.evaluation.evaluator INFO: Inference done 155/5000. Dataloading: 0.0012 s/iter. Inference: 0.2467 s/iter. Eval: 0.0009 s/iter. Total: 0.2488 s/iter. ETA=0:20:05
[10/01 17:15:06] d2.evaluation.evaluator INFO: Inference done 176/5000. Dataloading: 0.0012 s/iter. Inference: 0.2460 s/iter. Eval: 0.0010 s/iter. Total: 0.2482 s/iter. ETA=0:19:57
[10/01 17:15:11] d2.evaluation.evaluator INFO: Inference done 197/5000. Dataloading: 0.0012 s/iter. Inference: 0.2457 s/iter. Eval: 0.0009 s/iter. Total: 0.2479 s/iter. ETA=0:19:50
[10/01 17:15:16] d2.evaluation.evaluator INFO: Inference done 218/5000. Dataloading: 0.0012 s/iter. Inference: 0.2456 s/iter. Eval: 0.0008 s/iter. Total: 0.2477 s/iter. ETA=0:19:44
[10/01 17:15:21] d2.evaluation.evaluator INFO: Inference done 239/5000. Dataloading: 0.0012 s/iter. Inference: 0.2456 s/iter. Eval: 0.0009 s/iter. Total: 0.2477 s/iter. ETA=0:19:39
[10/01 17:15:26] d2.evaluation.evaluator INFO: Inference done 259/5000. Dataloading: 0.0012 s/iter. Inference: 0.2460 s/iter. Eval: 0.0008 s/iter. Total: 0.2481 s/iter. ETA=0:19:36
[10/01 17:15:31] d2.evaluation.evaluator INFO: Inference done 280/5000. Dataloading: 0.0012 s/iter. Inference: 0.2455 s/iter. Eval: 0.0009 s/iter. Total: 0.2477 s/iter. ETA=0:19:29
[10/01 17:15:36] d2.evaluation.evaluator INFO: Inference done 300/5000. Dataloading: 0.0012 s/iter. Inference: 0.2458 s/iter. Eval: 0.0010 s/iter. Total: 0.2480 s/iter. ETA=0:19:25
[10/01 17:15:42] d2.evaluation.evaluator INFO: Inference done 321/5000. Dataloading: 0.0012 s/iter. Inference: 0.2457 s/iter. Eval: 0.0009 s/iter. Total: 0.2478 s/iter. ETA=0:19:19
[10/01 17:15:47] d2.evaluation.evaluator INFO: Inference done 341/5000. Dataloading: 0.0012 s/iter. Inference: 0.2460 s/iter. Eval: 0.0009 s/iter. Total: 0.2481 s/iter. ETA=0:19:16
[10/01 17:15:52] d2.evaluation.evaluator INFO: Inference done 361/5000. Dataloading: 0.0012 s/iter. Inference: 0.2462 s/iter. Eval: 0.0009 s/iter. Total: 0.2484 s/iter. ETA=0:19:12
[10/01 17:15:57] d2.evaluation.evaluator INFO: Inference done 382/5000. Dataloading: 0.0012 s/iter. Inference: 0.2461 s/iter. Eval: 0.0009 s/iter. Total: 0.2483 s/iter. ETA=0:19:06
[10/01 17:16:02] d2.evaluation.evaluator INFO: Inference done 403/5000. Dataloading: 0.0012 s/iter. Inference: 0.2458 s/iter. Eval: 0.0009 s/iter. Total: 0.2480 s/iter. ETA=0:19:00
[10/01 17:16:07] d2.evaluation.evaluator INFO: Inference done 424/5000. Dataloading: 0.0012 s/iter. Inference: 0.2455 s/iter. Eval: 0.0009 s/iter. Total: 0.2477 s/iter. ETA=0:18:53
[10/01 17:16:12] d2.evaluation.evaluator INFO: Inference done 444/5000. Dataloading: 0.0012 s/iter. Inference: 0.2458 s/iter. Eval: 0.0009 s/iter. Total: 0.2480 s/iter. ETA=0:18:49
[10/01 17:16:17] d2.evaluation.evaluator INFO: Inference done 464/5000. Dataloading: 0.0012 s/iter. Inference: 0.2460 s/iter. Eval: 0.0009 s/iter. Total: 0.2482 s/iter. ETA=0:18:45
[10/01 17:16:22] d2.evaluation.evaluator INFO: Inference done 484/5000. Dataloading: 0.0012 s/iter. Inference: 0.2462 s/iter. Eval: 0.0009 s/iter. Total: 0.2484 s/iter. ETA=0:18:41
[10/01 17:16:27] d2.evaluation.evaluator INFO: Inference done 505/5000. Dataloading: 0.0012 s/iter. Inference: 0.2459 s/iter. Eval: 0.0009 s/iter. Total: 0.2481 s/iter. ETA=0:18:35
[10/01 17:16:32] d2.evaluation.evaluator INFO: Inference done 526/5000. Dataloading: 0.0012 s/iter. Inference: 0.2459 s/iter. Eval: 0.0009 s/iter. Total: 0.2480 s/iter. ETA=0:18:29
[10/01 17:16:38] d2.evaluation.evaluator INFO: Inference done 547/5000. Dataloading: 0.0012 s/iter. Inference: 0.2458 s/iter. Eval: 0.0009 s/iter. Total: 0.2480 s/iter. ETA=0:18:24
[10/01 17:20:33] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:20:33] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:20:33] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:20:33] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=256,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:20:33] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:20:33] d2.utils.env INFO: Using a generated random seed 37757619
[10/01 17:20:34] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:20:34] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:20:34] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:20:34] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:20:35] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:20:35] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:20:35] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:20:35] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:20:35] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:20:36] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:20:36] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:20:36] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:20:39] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0008 s/iter. Inference: 0.2282 s/iter. Eval: 0.0001 s/iter. Total: 0.2291 s/iter. ETA=0:19:02
[10/01 17:20:45] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0012 s/iter. Inference: 0.2335 s/iter. Eval: 0.0001 s/iter. Total: 0.2348 s/iter. ETA=0:19:26
[10/01 17:20:50] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0012 s/iter. Inference: 0.2338 s/iter. Eval: 0.0001 s/iter. Total: 0.2351 s/iter. ETA=0:19:22
[10/01 17:20:55] d2.evaluation.evaluator INFO: Inference done 76/5000. Dataloading: 0.0013 s/iter. Inference: 0.2350 s/iter. Eval: 0.0001 s/iter. Total: 0.2364 s/iter. ETA=0:19:23
[10/01 17:21:00] d2.evaluation.evaluator INFO: Inference done 98/5000. Dataloading: 0.0012 s/iter. Inference: 0.2331 s/iter. Eval: 0.0001 s/iter. Total: 0.2344 s/iter. ETA=0:19:09
[10/01 17:21:05] d2.evaluation.evaluator INFO: Inference done 120/5000. Dataloading: 0.0013 s/iter. Inference: 0.2331 s/iter. Eval: 0.0001 s/iter. Total: 0.2345 s/iter. ETA=0:19:04
[10/01 17:21:10] d2.evaluation.evaluator INFO: Inference done 141/5000. Dataloading: 0.0013 s/iter. Inference: 0.2356 s/iter. Eval: 0.0001 s/iter. Total: 0.2370 s/iter. ETA=0:19:11
[10/01 17:21:15] d2.evaluation.evaluator INFO: Inference done 162/5000. Dataloading: 0.0013 s/iter. Inference: 0.2370 s/iter. Eval: 0.0001 s/iter. Total: 0.2384 s/iter. ETA=0:19:13
[10/01 17:21:20] d2.evaluation.evaluator INFO: Inference done 183/5000. Dataloading: 0.0013 s/iter. Inference: 0.2372 s/iter. Eval: 0.0001 s/iter. Total: 0.2386 s/iter. ETA=0:19:09
[10/01 17:21:26] d2.evaluation.evaluator INFO: Inference done 204/5000. Dataloading: 0.0013 s/iter. Inference: 0.2377 s/iter. Eval: 0.0001 s/iter. Total: 0.2391 s/iter. ETA=0:19:06
[10/01 17:21:31] d2.evaluation.evaluator INFO: Inference done 225/5000. Dataloading: 0.0013 s/iter. Inference: 0.2386 s/iter. Eval: 0.0001 s/iter. Total: 0.2400 s/iter. ETA=0:19:05
[10/01 17:21:36] d2.evaluation.evaluator INFO: Inference done 245/5000. Dataloading: 0.0013 s/iter. Inference: 0.2394 s/iter. Eval: 0.0001 s/iter. Total: 0.2408 s/iter. ETA=0:19:05
[10/01 17:21:41] d2.evaluation.evaluator INFO: Inference done 266/5000. Dataloading: 0.0013 s/iter. Inference: 0.2397 s/iter. Eval: 0.0001 s/iter. Total: 0.2411 s/iter. ETA=0:19:01
[10/01 17:21:46] d2.evaluation.evaluator INFO: Inference done 287/5000. Dataloading: 0.0013 s/iter. Inference: 0.2399 s/iter. Eval: 0.0001 s/iter. Total: 0.2413 s/iter. ETA=0:18:57
[10/01 17:21:51] d2.evaluation.evaluator INFO: Inference done 308/5000. Dataloading: 0.0013 s/iter. Inference: 0.2402 s/iter. Eval: 0.0001 s/iter. Total: 0.2416 s/iter. ETA=0:18:53
[10/01 17:21:56] d2.evaluation.evaluator INFO: Inference done 329/5000. Dataloading: 0.0013 s/iter. Inference: 0.2405 s/iter. Eval: 0.0001 s/iter. Total: 0.2419 s/iter. ETA=0:18:49
[10/01 17:22:02] d2.evaluation.evaluator INFO: Inference done 350/5000. Dataloading: 0.0013 s/iter. Inference: 0.2410 s/iter. Eval: 0.0001 s/iter. Total: 0.2424 s/iter. ETA=0:18:47
[10/01 17:22:07] d2.evaluation.evaluator INFO: Inference done 371/5000. Dataloading: 0.0013 s/iter. Inference: 0.2414 s/iter. Eval: 0.0001 s/iter. Total: 0.2428 s/iter. ETA=0:18:43
[10/01 17:22:12] d2.evaluation.evaluator INFO: Inference done 393/5000. Dataloading: 0.0013 s/iter. Inference: 0.2410 s/iter. Eval: 0.0001 s/iter. Total: 0.2424 s/iter. ETA=0:18:36
[10/01 17:22:17] d2.evaluation.evaluator INFO: Inference done 414/5000. Dataloading: 0.0013 s/iter. Inference: 0.2411 s/iter. Eval: 0.0001 s/iter. Total: 0.2425 s/iter. ETA=0:18:32
[10/01 17:22:22] d2.evaluation.evaluator INFO: Inference done 435/5000. Dataloading: 0.0013 s/iter. Inference: 0.2412 s/iter. Eval: 0.0001 s/iter. Total: 0.2426 s/iter. ETA=0:18:27
[10/01 17:22:27] d2.evaluation.evaluator INFO: Inference done 456/5000. Dataloading: 0.0013 s/iter. Inference: 0.2413 s/iter. Eval: 0.0001 s/iter. Total: 0.2427 s/iter. ETA=0:18:22
[10/01 17:22:33] d2.evaluation.evaluator INFO: Inference done 476/5000. Dataloading: 0.0013 s/iter. Inference: 0.2418 s/iter. Eval: 0.0001 s/iter. Total: 0.2432 s/iter. ETA=0:18:20
[10/01 17:22:38] d2.evaluation.evaluator INFO: Inference done 497/5000. Dataloading: 0.0013 s/iter. Inference: 0.2420 s/iter. Eval: 0.0001 s/iter. Total: 0.2434 s/iter. ETA=0:18:15
[10/01 17:22:43] d2.evaluation.evaluator INFO: Inference done 519/5000. Dataloading: 0.0013 s/iter. Inference: 0.2416 s/iter. Eval: 0.0001 s/iter. Total: 0.2430 s/iter. ETA=0:18:08
[10/01 17:22:48] d2.evaluation.evaluator INFO: Inference done 540/5000. Dataloading: 0.0013 s/iter. Inference: 0.2418 s/iter. Eval: 0.0001 s/iter. Total: 0.2432 s/iter. ETA=0:18:04
[10/01 17:30:07] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:30:07] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:30:08] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:30:08] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:30:08] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:30:08] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:30:08] d2.utils.env INFO: Using a generated random seed 12134351
[10/01 17:30:08] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:30:08] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:30:08] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:30:08] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:30:08] d2.utils.env INFO: Using a generated random seed 12222071
[10/01 17:30:09] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:30:09] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:30:09] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:30:09] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:30:09] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:30:09] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:30:09] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:30:09] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:30:10] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:30:10] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:30:10] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:30:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:30:10] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:30:10] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:30:10] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:30:10] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:30:10] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:30:10] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:30:10] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:30:10] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:30:10] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:30:10] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:30:11] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:30:11] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:30:17] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0008 s/iter. Inference: 0.4757 s/iter. Eval: 0.0003 s/iter. Total: 0.4768 s/iter. ETA=0:39:38
[10/01 17:30:17] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0010 s/iter. Inference: 0.4680 s/iter. Eval: 0.0002 s/iter. Total: 0.4693 s/iter. ETA=0:39:01
[10/01 17:30:22] d2.evaluation.evaluator INFO: Inference done 22/5000. Dataloading: 0.0011 s/iter. Inference: 0.4659 s/iter. Eval: 0.0001 s/iter. Total: 0.4672 s/iter. ETA=0:38:45
[10/01 17:30:22] d2.evaluation.evaluator INFO: Inference done 22/5000. Dataloading: 0.0011 s/iter. Inference: 0.4656 s/iter. Eval: 0.0001 s/iter. Total: 0.4669 s/iter. ETA=0:38:44
[10/01 17:30:27] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0012 s/iter. Inference: 0.4727 s/iter. Eval: 0.0001 s/iter. Total: 0.4740 s/iter. ETA=0:39:14
[10/01 17:30:27] d2.evaluation.evaluator INFO: Inference done 32/5000. Dataloading: 0.0012 s/iter. Inference: 0.4810 s/iter. Eval: 0.0007 s/iter. Total: 0.4830 s/iter. ETA=0:39:59
[10/01 17:30:32] d2.evaluation.evaluator INFO: Inference done 44/5000. Dataloading: 0.0012 s/iter. Inference: 0.4736 s/iter. Eval: 0.0001 s/iter. Total: 0.4750 s/iter. ETA=0:39:13
[10/01 17:30:32] d2.evaluation.evaluator INFO: Inference done 43/5000. Dataloading: 0.0012 s/iter. Inference: 0.4758 s/iter. Eval: 0.0008 s/iter. Total: 0.4779 s/iter. ETA=0:39:28
[10/01 17:30:38] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0012 s/iter. Inference: 0.4743 s/iter. Eval: 0.0001 s/iter. Total: 0.4757 s/iter. ETA=0:39:12
[10/01 17:30:38] d2.evaluation.evaluator INFO: Inference done 54/5000. Dataloading: 0.0013 s/iter. Inference: 0.4746 s/iter. Eval: 0.0006 s/iter. Total: 0.4766 s/iter. ETA=0:39:17
[10/01 17:30:43] d2.evaluation.evaluator INFO: Inference done 64/5000. Dataloading: 0.0013 s/iter. Inference: 0.4788 s/iter. Eval: 0.0007 s/iter. Total: 0.4807 s/iter. ETA=0:39:32
[10/01 17:30:43] d2.evaluation.evaluator INFO: Inference done 66/5000. Dataloading: 0.0012 s/iter. Inference: 0.4788 s/iter. Eval: 0.0001 s/iter. Total: 0.4802 s/iter. ETA=0:39:29
[10/01 17:30:48] d2.evaluation.evaluator INFO: Inference done 74/5000. Dataloading: 0.0013 s/iter. Inference: 0.4834 s/iter. Eval: 0.0007 s/iter. Total: 0.4854 s/iter. ETA=0:39:51
[10/01 17:30:48] d2.evaluation.evaluator INFO: Inference done 76/5000. Dataloading: 0.0012 s/iter. Inference: 0.4830 s/iter. Eval: 0.0001 s/iter. Total: 0.4844 s/iter. ETA=0:39:45
[10/01 17:30:53] d2.evaluation.evaluator INFO: Inference done 84/5000. Dataloading: 0.0013 s/iter. Inference: 0.4856 s/iter. Eval: 0.0006 s/iter. Total: 0.4876 s/iter. ETA=0:39:57
[10/01 17:30:53] d2.evaluation.evaluator INFO: Inference done 87/5000. Dataloading: 0.0013 s/iter. Inference: 0.4829 s/iter. Eval: 0.0001 s/iter. Total: 0.4844 s/iter. ETA=0:39:39
[10/01 17:30:58] d2.evaluation.evaluator INFO: Inference done 94/5000. Dataloading: 0.0013 s/iter. Inference: 0.4882 s/iter. Eval: 0.0006 s/iter. Total: 0.4901 s/iter. ETA=0:40:04
[10/01 17:30:59] d2.evaluation.evaluator INFO: Inference done 98/5000. Dataloading: 0.0012 s/iter. Inference: 0.4821 s/iter. Eval: 0.0001 s/iter. Total: 0.4836 s/iter. ETA=0:39:30
[10/01 17:31:03] d2.evaluation.evaluator INFO: Inference done 105/5000. Dataloading: 0.0013 s/iter. Inference: 0.4878 s/iter. Eval: 0.0005 s/iter. Total: 0.4897 s/iter. ETA=0:39:56
[10/01 17:31:04] d2.evaluation.evaluator INFO: Inference done 109/5000. Dataloading: 0.0012 s/iter. Inference: 0.4830 s/iter. Eval: 0.0001 s/iter. Total: 0.4844 s/iter. ETA=0:39:29
[10/01 17:31:09] d2.evaluation.evaluator INFO: Inference done 116/5000. Dataloading: 0.0013 s/iter. Inference: 0.4892 s/iter. Eval: 0.0005 s/iter. Total: 0.4911 s/iter. ETA=0:39:58
[10/01 17:31:09] d2.evaluation.evaluator INFO: Inference done 119/5000. Dataloading: 0.0012 s/iter. Inference: 0.4853 s/iter. Eval: 0.0001 s/iter. Total: 0.4867 s/iter. ETA=0:39:35
[10/01 17:31:14] d2.evaluation.evaluator INFO: Inference done 126/5000. Dataloading: 0.0013 s/iter. Inference: 0.4909 s/iter. Eval: 0.0005 s/iter. Total: 0.4927 s/iter. ETA=0:40:01
[10/01 17:31:14] d2.evaluation.evaluator INFO: Inference done 129/5000. Dataloading: 0.0013 s/iter. Inference: 0.4873 s/iter. Eval: 0.0001 s/iter. Total: 0.4887 s/iter. ETA=0:39:40
[10/01 17:31:19] d2.evaluation.evaluator INFO: Inference done 136/5000. Dataloading: 0.0013 s/iter. Inference: 0.4937 s/iter. Eval: 0.0005 s/iter. Total: 0.4955 s/iter. ETA=0:40:10
[10/01 17:31:20] d2.evaluation.evaluator INFO: Inference done 139/5000. Dataloading: 0.0013 s/iter. Inference: 0.4902 s/iter. Eval: 0.0001 s/iter. Total: 0.4916 s/iter. ETA=0:39:49
[10/01 17:31:24] d2.evaluation.evaluator INFO: Inference done 146/5000. Dataloading: 0.0013 s/iter. Inference: 0.4962 s/iter. Eval: 0.0004 s/iter. Total: 0.4980 s/iter. ETA=0:40:17
[10/01 17:31:25] d2.evaluation.evaluator INFO: Inference done 149/5000. Dataloading: 0.0013 s/iter. Inference: 0.4927 s/iter. Eval: 0.0001 s/iter. Total: 0.4941 s/iter. ETA=0:39:57
[10/01 17:31:30] d2.evaluation.evaluator INFO: Inference done 156/5000. Dataloading: 0.0013 s/iter. Inference: 0.4978 s/iter. Eval: 0.0004 s/iter. Total: 0.4996 s/iter. ETA=0:40:20
[10/01 17:31:30] d2.evaluation.evaluator INFO: Inference done 159/5000. Dataloading: 0.0013 s/iter. Inference: 0.4946 s/iter. Eval: 0.0001 s/iter. Total: 0.4961 s/iter. ETA=0:40:01
[10/01 17:31:35] d2.evaluation.evaluator INFO: Inference done 172/5000. Dataloading: 0.0013 s/iter. Inference: 0.4869 s/iter. Eval: 0.0001 s/iter. Total: 0.4883 s/iter. ETA=0:39:17
[10/01 17:34:15] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:34:15] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:34:15] detectron2 INFO: Rank of current process: 0. World size: 1
[10/01 17:34:15] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:34:15] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:34:15] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:34:15] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:34:15] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:34:15] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:34:15] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:34:15] d2.utils.env INFO: Using a generated random seed 19946315
[10/01 17:34:15] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:34:15] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:34:15] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:34:15] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:34:15] d2.utils.env INFO: Using a generated random seed 19986199
[10/01 17:34:15] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:34:15] d2.utils.env INFO: Using a generated random seed 19997542
[10/01 17:34:17] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:34:17] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:34:17] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:34:17] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:34:17] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:34:17] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:34:17] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:34:17] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:34:17] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:34:17] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:34:17] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:34:17] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:34:18] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:34:18] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:34:18] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:34:18] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:34:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:34:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:34:18] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:34:18] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:34:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:34:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:34:18] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:34:18] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:34:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:34:18] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:34:18] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:34:18] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:34:18] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:34:18] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:34:18] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:34:18] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:34:18] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:34:19] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:34:19] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:34:19] d2.evaluation.evaluator INFO: Start inference on 5000 batches
[10/01 17:34:27] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0009 s/iter. Inference: 0.7007 s/iter. Eval: 0.0001 s/iter. Total: 0.7017 s/iter. ETA=0:58:20
[10/01 17:34:28] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0011 s/iter. Inference: 0.7030 s/iter. Eval: 0.0001 s/iter. Total: 0.7041 s/iter. ETA=0:58:32
[10/01 17:34:28] d2.evaluation.evaluator INFO: Inference done 11/5000. Dataloading: 0.0010 s/iter. Inference: 0.7001 s/iter. Eval: 0.0001 s/iter. Total: 0.7012 s/iter. ETA=0:58:18
[10/01 17:34:33] d2.evaluation.evaluator INFO: Inference done 19/5000. Dataloading: 0.0012 s/iter. Inference: 0.6838 s/iter. Eval: 0.0001 s/iter. Total: 0.6852 s/iter. ETA=0:56:52
[10/01 17:34:33] d2.evaluation.evaluator INFO: Inference done 19/5000. Dataloading: 0.0012 s/iter. Inference: 0.6842 s/iter. Eval: 0.0001 s/iter. Total: 0.6855 s/iter. ETA=0:56:54
[10/01 17:34:33] d2.evaluation.evaluator INFO: Inference done 19/5000. Dataloading: 0.0011 s/iter. Inference: 0.6825 s/iter. Eval: 0.0001 s/iter. Total: 0.6838 s/iter. ETA=0:56:46
[10/01 17:34:38] d2.evaluation.evaluator INFO: Inference done 26/5000. Dataloading: 0.0012 s/iter. Inference: 0.7058 s/iter. Eval: 0.0001 s/iter. Total: 0.7072 s/iter. ETA=0:58:37
[10/01 17:34:38] d2.evaluation.evaluator INFO: Inference done 26/5000. Dataloading: 0.0012 s/iter. Inference: 0.7060 s/iter. Eval: 0.0001 s/iter. Total: 0.7074 s/iter. ETA=0:58:38
[10/01 17:34:38] d2.evaluation.evaluator INFO: Inference done 26/5000. Dataloading: 0.0012 s/iter. Inference: 0.7049 s/iter. Eval: 0.0002 s/iter. Total: 0.7063 s/iter. ETA=0:58:33
[10/01 17:34:43] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0012 s/iter. Inference: 0.7168 s/iter. Eval: 0.0001 s/iter. Total: 0.7182 s/iter. ETA=0:59:27
[10/01 17:34:43] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0013 s/iter. Inference: 0.7161 s/iter. Eval: 0.0001 s/iter. Total: 0.7176 s/iter. ETA=0:59:24
[10/01 17:34:44] d2.evaluation.evaluator INFO: Inference done 33/5000. Dataloading: 0.0012 s/iter. Inference: 0.7163 s/iter. Eval: 0.0001 s/iter. Total: 0.7177 s/iter. ETA=0:59:24
[10/01 17:34:48] d2.evaluation.evaluator INFO: Inference done 40/5000. Dataloading: 0.0012 s/iter. Inference: 0.7183 s/iter. Eval: 0.0001 s/iter. Total: 0.7197 s/iter. ETA=0:59:29
[10/01 17:34:48] d2.evaluation.evaluator INFO: Inference done 40/5000. Dataloading: 0.0013 s/iter. Inference: 0.7170 s/iter. Eval: 0.0001 s/iter. Total: 0.7185 s/iter. ETA=0:59:23
[10/01 17:34:49] d2.evaluation.evaluator INFO: Inference done 40/5000. Dataloading: 0.0012 s/iter. Inference: 0.7191 s/iter. Eval: 0.0002 s/iter. Total: 0.7206 s/iter. ETA=0:59:34
[10/01 17:34:53] d2.evaluation.evaluator INFO: Inference done 47/5000. Dataloading: 0.0013 s/iter. Inference: 0.7199 s/iter. Eval: 0.0001 s/iter. Total: 0.7214 s/iter. ETA=0:59:32
[10/01 17:34:54] d2.evaluation.evaluator INFO: Inference done 47/5000. Dataloading: 0.0013 s/iter. Inference: 0.7184 s/iter. Eval: 0.0001 s/iter. Total: 0.7198 s/iter. ETA=0:59:25
[10/01 17:34:54] d2.evaluation.evaluator INFO: Inference done 47/5000. Dataloading: 0.0012 s/iter. Inference: 0.7225 s/iter. Eval: 0.0002 s/iter. Total: 0.7240 s/iter. ETA=0:59:45
[10/01 17:34:58] d2.evaluation.evaluator INFO: Inference done 54/5000. Dataloading: 0.0013 s/iter. Inference: 0.7234 s/iter. Eval: 0.0001 s/iter. Total: 0.7248 s/iter. ETA=0:59:44
[10/01 17:34:59] d2.evaluation.evaluator INFO: Inference done 55/5000. Dataloading: 0.0013 s/iter. Inference: 0.7126 s/iter. Eval: 0.0001 s/iter. Total: 0.7140 s/iter. ETA=0:58:50
[10/01 17:44:38] detectron2 INFO: Rank of current process: 0. World size: 2
[10/01 17:44:38] detectron2 INFO: Environment info:
-------------------------------  ------------------------------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]
numpy                            1.26.4
detectron2                       0.6 @/home/lynuc/hiera_multi_scale/detectron2-hiera-MoE-hiera/detectron2
detectron2._C                    not built correctly: No module named 'detectron2._C'
Compiler ($CXX)                  c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
CUDA compiler                    Not found
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          2.4.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0,1                          NVIDIA GeForce GTX 1080 (arch=6.1)
Driver version                   550.78
CUDA_HOME                        /usr/local/cuda
Pillow                           10.4.0
torchvision                      0.19.1 @/home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision
torchvision arch flags           /home/lynuc/miniconda3/envs/test/lib/python3.12/site-packages/torchvision/_C.so; cannot find cuobjdump
fvcore                           0.1.5.post20221221
iopath                           0.1.10
cv2                              4.10.0
-------------------------------  ------------------------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.4.2 (Git Hash 1137e04ec0b5251ca2b4400a4fd3c667ce843d67)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1  (built against CUDA 12.4)
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.4.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

[10/01 17:44:38] detectron2 INFO: Command line arguments: Namespace(config_file='./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py', resume=False, eval_only=True, num_gpus=2, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50175', opts=['train.init_checkpoint=/home/lynuc/model_0079999.pth'])
[10/01 17:44:38] detectron2 INFO: Contents of args.config_file=./projects/HieraDet/configs/mask_rcnn_hieradet_b_plus_512.py:
from functools import partial
from fvcore.common.param_scheduler import MultiStepParamScheduler

from detectron2 import model_zoo
from detectron2.config import LazyCall as L
from detectron2.layers import ShapeSpec
from detectron2.modeling.meta_arch import GeneralizedRCNN
from detectron2.modeling.anchor_generator import DefaultAnchorGenerator
from detectron2.modeling.backbone.fpn import LastLevelMaxPool
from detectron2.modeling.backbone import BasicStem, FPN, ResNet
from detectron2.modeling.box_regression import Box2BoxTransform
from detectron2.modeling.matcher import Matcher
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.roi_heads import (
    StandardROIHeads,
    FastRCNNOutputLayers,
    MaskRCNNConvUpsampleHead,
    FastRCNNConvFCHead,
)

from detectron2.modeling.proposal_generator import RPN, StandardRPNHead

from detectron2.solver import WarmupParamScheduler
from detectron2.modeling.backbone.vit import get_vit_lr_decay_rate
from detectron2.modeling.backbone import HieraDet, FpnNeck_HieraDet, ImageEncoder_HieraDet

from sam2.modeling.position_encoding import PositionEmbeddingSine

from .common.coco_loader_lsj import dataloader

constants = model_zoo.get_config("common/data/constants.py").constants


model = L(GeneralizedRCNN)(
    backbone=L(ImageEncoder_HieraDet)(
        trunk=L(HieraDet)(
            embed_dim=112,
            num_heads=2,
            stages=(2, 3, 16, 3),
        ),
        neck=L(FpnNeck_HieraDet)(
            position_encoding=PositionEmbeddingSine(
                num_pos_feats=256,
                normalize=True,
                temperature=10000,
            ),
            d_model=256,
            backbone_channel_list=[896, 448, 224, 112],
            fpn_top_down_levels= [2, 3],
            fpn_interp_model = 'bicubic'
        )
    ),
    proposal_generator=L(RPN)(
        in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        head=L(StandardRPNHead)(in_channels=256, num_anchors=3),
        anchor_generator=L(DefaultAnchorGenerator)(
            sizes=[[32], [64], [128], [256], [512]],
            aspect_ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32, 64],
            offset=0.0,
        ),
        anchor_matcher=L(Matcher)(
            thresholds=[0.3, 0.7], labels=[0, -1, 1], allow_low_quality_matches=True
        ),
        box2box_transform=L(Box2BoxTransform)(weights=[1.0, 1.0, 1.0, 1.0]),
        batch_size_per_image=256,
        positive_fraction=0.5,
        pre_nms_topk=(2000, 1000),
        post_nms_topk=(1000, 1000),
        nms_thresh=0.7,
    ),
    roi_heads=L(StandardROIHeads)(
        num_classes=80,
        batch_size_per_image=512,
        positive_fraction=0.25,
        proposal_matcher=L(Matcher)(
            thresholds=[0.5], labels=[0, 1], allow_low_quality_matches=False
        ),
        box_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        box_pooler=L(ROIPooler)(
            output_size=7,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        box_head=L(FastRCNNConvFCHead)(
            input_shape=ShapeSpec(channels=256, height=7, width=7),
            conv_dims=[],
            fc_dims=[1024, 1024],
        ),
        box_predictor=L(FastRCNNOutputLayers)(
            input_shape=ShapeSpec(channels=1024),
            test_score_thresh=0.05,
            box2box_transform=L(Box2BoxTransform)(weights=(10, 10, 5, 5)),
            num_classes="${..num_classes}",
        ),
        mask_in_features=["stage_0", "stage_1", "stage_2", "stage_3"],
        mask_pooler=L(ROIPooler)(
            output_size=14,
            scales=(1.0 / 4, 1.0 / 8, 1.0 / 16, 1.0 / 32),
            sampling_ratio=0,
            pooler_type="ROIAlignV2",
        ),
        mask_head=L(MaskRCNNConvUpsampleHead)(
            input_shape=ShapeSpec(channels=256, width=14, height=14),
            num_classes="${..num_classes}",
            conv_dims=[256, 256, 256, 256, 256],
        ),
    ),
    pixel_mean=constants.imagenet_rgb256_mean,
    pixel_std=constants.imagenet_rgb256_std,
    input_format="RGB",
)

# Initialization and trainer settings
train = model_zoo.get_config("common/train.py").train
train.amp.enabled = True
train.ddp.fp16_compression = True
#train.init_checkpoint = "/media/Pluto/andy/detectron2/projects/Hiera/init_ckpt/hiera_official/mae_hiera_tiny_224-model_state.pth"
# train.output_dir = "./output/mask_rcnn_hieradet_b_plus_512"
# train.trunk_ckpt = '/home/s108061519/hiera_moe/logs/in1k_mae/mae_hiera_base_plus_512/epoch-epoch=259.ckpt'
dataloader.train.total_batch_size = 1
train.init_checkpoint = "/home/lynuc/model_0079999.pth"
train.output_dir = "/home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512"
train.trunk_ckpt = '/home/lynuc/last.ckpt'


# 36 epochs
train.max_iter = 67500 * 64 // dataloader.train.total_batch_size
lr_multiplier = L(WarmupParamScheduler)(
    scheduler=L(MultiStepParamScheduler)(
        values=[1.0, 0.1, 0.01],
        milestones=[52500, 62500, 67500],
    ),
    warmup_length=250 / train.max_iter,
    warmup_factor=0.001,
)

optimizer = model_zoo.get_config("common/optim.py").AdamW
optimizer.params.overrides = {
    "pos_embed": {"weight_decay": 0.0},
    "rel_pos_h": {"weight_decay": 0.0},
    "rel_pos_w": {"weight_decay": 0.0},
}
optimizer.lr = 1.6e-4

[10/01 17:44:38] detectron2 INFO: Full config saved to /home/lynuc/hiera_multi_scale/output/mask_rcnn_hieradet_b_plus_512/config.yaml
[10/01 17:44:38] d2.utils.env INFO: Using a generated random seed 42750771
[10/01 17:44:39] d2.checkpoint.detection_checkpoint INFO: [DetectionCheckpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:44:39] fvcore.common.checkpoint INFO: [Checkpointer] Loading from /home/lynuc/model_0079999.pth ...
[10/01 17:44:39] fvcore.common.checkpoint WARNING: Some model parameters or buffers are not found in the checkpoint:
[34mbackbone.neck.convs.0.conv.{bias, weight}[0m
[34mbackbone.neck.convs.1.conv.{bias, weight}[0m
[34mbackbone.neck.convs.2.conv.{bias, weight}[0m
[34mbackbone.neck.convs.3.conv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.0.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.1.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.10.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.11.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.12.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.13.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.14.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.15.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.16.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.17.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.18.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.19.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.2.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.20.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.21.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.22.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.23.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.3.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.4.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.5.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.6.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.7.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.8.norm2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.proj.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.attn.qkv.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.mlp.fc2.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm1.{bias, weight}[0m
[34mbackbone.trunk.blocks.9.norm2.{bias, weight}[0m
[34mbackbone.trunk.patch_embed.proj.{bias, weight}[0m
[34mbackbone.trunk.{pos_embed, pos_embed_window}[0m
[10/01 17:44:39] fvcore.common.checkpoint WARNING: The checkpoint state_dict contains keys that are not used by the model:
  [35mbackbone.fpn_lateral2.{bias, weight}[0m
  [35mbackbone.fpn_output2.{bias, weight}[0m
  [35mbackbone.fpn_lateral3.{bias, weight}[0m
  [35mbackbone.fpn_output3.{bias, weight}[0m
  [35mbackbone.fpn_lateral4.{bias, weight}[0m
  [35mbackbone.fpn_output4.{bias, weight}[0m
  [35mbackbone.fpn_lateral5.{bias, weight}[0m
  [35mbackbone.fpn_output5.{bias, weight}[0m
  [35mbackbone.bottom_up.{pos_embed, pos_embed_window}[0m
  [35mbackbone.bottom_up.patch_embed.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.0.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.1.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.2.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.3.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.4.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.5.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.6.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.7.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.8.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.9.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.mlp.fc2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.10.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.qkv.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.attn.proj.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.norm2.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc1.{bias, weight}[0m
  [35mbackbone.bottom_up.blocks.11.mlp.fc2.{bias, weight}[0m
[10/01 17:44:40] d2.data.datasets.coco INFO: Loaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[10/01 17:44:40] d2.data.build INFO: Distribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[10/01 17:44:40] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333)]
[10/01 17:44:40] d2.data.common INFO: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[10/01 17:44:40] d2.data.common INFO: Serializing 5000 elements to byte tensors and concatenating them all ...
[10/01 17:44:43] d2.data.common INFO: Serialized dataset takes 19.10 MiB
[10/01 17:44:43] d2.evaluation.coco_evaluation INFO: Fast COCO eval is not built. Falling back to official COCO eval.
[10/01 17:44:43] d2.evaluation.evaluator INFO: Start inference on 2500 batches
[10/01 17:44:50] d2.evaluation.evaluator INFO: Inference done 11/2500. Dataloading: 0.0009 s/iter. Inference: 0.2307 s/iter. Eval: 0.0001 s/iter. Total: 0.2317 s/iter. ETA=0:09:36
[10/01 17:44:55] d2.evaluation.evaluator INFO: Inference done 33/2500. Dataloading: 0.0012 s/iter. Inference: 0.2360 s/iter. Eval: 0.0002 s/iter. Total: 0.2375 s/iter. ETA=0:09:45
[10/01 17:45:00] d2.evaluation.evaluator INFO: Inference done 54/2500. Dataloading: 0.0012 s/iter. Inference: 0.2406 s/iter. Eval: 0.0003 s/iter. Total: 0.2422 s/iter. ETA=0:09:52
[10/01 17:45:05] d2.evaluation.evaluator INFO: Inference done 75/2500. Dataloading: 0.0014 s/iter. Inference: 0.2407 s/iter. Eval: 0.0003 s/iter. Total: 0.2424 s/iter. ETA=0:09:47
[10/01 17:45:11] d2.evaluation.evaluator INFO: Inference done 97/2500. Dataloading: 0.0014 s/iter. Inference: 0.2387 s/iter. Eval: 0.0003 s/iter. Total: 0.2404 s/iter. ETA=0:09:37
[10/01 17:45:16] d2.evaluation.evaluator INFO: Inference done 119/2500. Dataloading: 0.0014 s/iter. Inference: 0.2380 s/iter. Eval: 0.0004 s/iter. Total: 0.2398 s/iter. ETA=0:09:31
[10/01 17:45:21] d2.evaluation.evaluator INFO: Inference done 140/2500. Dataloading: 0.0014 s/iter. Inference: 0.2389 s/iter. Eval: 0.0004 s/iter. Total: 0.2407 s/iter. ETA=0:09:28
[10/01 17:45:26] d2.evaluation.evaluator INFO: Inference done 160/2500. Dataloading: 0.0014 s/iter. Inference: 0.2405 s/iter. Eval: 0.0004 s/iter. Total: 0.2423 s/iter. ETA=0:09:26
[10/01 17:45:31] d2.evaluation.evaluator INFO: Inference done 181/2500. Dataloading: 0.0014 s/iter. Inference: 0.2400 s/iter. Eval: 0.0004 s/iter. Total: 0.2418 s/iter. ETA=0:09:20
[10/01 17:45:36] d2.evaluation.evaluator INFO: Inference done 201/2500. Dataloading: 0.0014 s/iter. Inference: 0.2408 s/iter. Eval: 0.0004 s/iter. Total: 0.2427 s/iter. ETA=0:09:17
[10/01 17:45:41] d2.evaluation.evaluator INFO: Inference done 222/2500. Dataloading: 0.0014 s/iter. Inference: 0.2409 s/iter. Eval: 0.0004 s/iter. Total: 0.2427 s/iter. ETA=0:09:12
[10/01 17:45:46] d2.evaluation.evaluator INFO: Inference done 243/2500. Dataloading: 0.0014 s/iter. Inference: 0.2414 s/iter. Eval: 0.0003 s/iter. Total: 0.2432 s/iter. ETA=0:09:08
[10/01 17:45:51] d2.evaluation.evaluator INFO: Inference done 263/2500. Dataloading: 0.0014 s/iter. Inference: 0.2420 s/iter. Eval: 0.0003 s/iter. Total: 0.2438 s/iter. ETA=0:09:05
[10/01 17:45:57] d2.evaluation.evaluator INFO: Inference done 284/2500. Dataloading: 0.0014 s/iter. Inference: 0.2420 s/iter. Eval: 0.0004 s/iter. Total: 0.2438 s/iter. ETA=0:09:00
[10/01 17:46:02] d2.evaluation.evaluator INFO: Inference done 304/2500. Dataloading: 0.0014 s/iter. Inference: 0.2425 s/iter. Eval: 0.0003 s/iter. Total: 0.2443 s/iter. ETA=0:08:56
[10/01 17:46:07] d2.evaluation.evaluator INFO: Inference done 325/2500. Dataloading: 0.0014 s/iter. Inference: 0.2427 s/iter. Eval: 0.0003 s/iter. Total: 0.2445 s/iter. ETA=0:08:51
[10/01 17:46:12] d2.evaluation.evaluator INFO: Inference done 345/2500. Dataloading: 0.0014 s/iter. Inference: 0.2432 s/iter. Eval: 0.0003 s/iter. Total: 0.2449 s/iter. ETA=0:08:47
[10/01 17:46:17] d2.evaluation.evaluator INFO: Inference done 366/2500. Dataloading: 0.0014 s/iter. Inference: 0.2434 s/iter. Eval: 0.0003 s/iter. Total: 0.2452 s/iter. ETA=0:08:43
[10/01 17:46:22] d2.evaluation.evaluator INFO: Inference done 387/2500. Dataloading: 0.0014 s/iter. Inference: 0.2434 s/iter. Eval: 0.0003 s/iter. Total: 0.2452 s/iter. ETA=0:08:38
[10/01 17:46:27] d2.evaluation.evaluator INFO: Inference done 408/2500. Dataloading: 0.0014 s/iter. Inference: 0.2434 s/iter. Eval: 0.0004 s/iter. Total: 0.2452 s/iter. ETA=0:08:32
[10/01 17:46:32] d2.evaluation.evaluator INFO: Inference done 429/2500. Dataloading: 0.0014 s/iter. Inference: 0.2434 s/iter. Eval: 0.0003 s/iter. Total: 0.2452 s/iter. ETA=0:08:27
[10/01 17:46:38] d2.evaluation.evaluator INFO: Inference done 449/2500. Dataloading: 0.0014 s/iter. Inference: 0.2438 s/iter. Eval: 0.0003 s/iter. Total: 0.2456 s/iter. ETA=0:08:23
[10/01 17:46:43] d2.evaluation.evaluator INFO: Inference done 469/2500. Dataloading: 0.0014 s/iter. Inference: 0.2441 s/iter. Eval: 0.0003 s/iter. Total: 0.2459 s/iter. ETA=0:08:19
[10/01 17:46:48] d2.evaluation.evaluator INFO: Inference done 489/2500. Dataloading: 0.0014 s/iter. Inference: 0.2444 s/iter. Eval: 0.0003 s/iter. Total: 0.2462 s/iter. ETA=0:08:15
